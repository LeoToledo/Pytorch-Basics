{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vamos estruturar nosso modelo para reconhecer gatos e cachorros. \n",
    "\n",
    "O dataset usado está disponível em https://www.microsoft.com/en-us/download/details.aspx?id=54765"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Pré-Processar o dataset pode ser um trabalho árduo.\n",
    "#A variável abaixo irá garantir que nós façamos isso o mínimo possível.\n",
    "REBUILD_DATA = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na classe abaixo definiremos variáveis que serão constantemente utilizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "REBUILD_DATA = True # set to true to one once, then back to false unless you want to change something in your training data.\n",
    "\n",
    "class DogsVSCats():\n",
    "    #As imagens do dataset possuem dimensões variáveis de uma para a outra\n",
    "    #Vamos padronizar o tamanho de todas as imagens para facilitar nossa vida\n",
    "    IMG_SIZE = 50\n",
    "    \n",
    "    #Definindo o path da pasta com o dataset\n",
    "    CATS = \"PetImages/Cat\"\n",
    "    DOGS = \"PetImages/Dog\"\n",
    "\n",
    "    #Definindo Labels\n",
    "    LABELS = {CATS: 0, DOGS: 1}\n",
    "    #Lista que guardará os dados de treino\n",
    "    training_data = []\n",
    "\n",
    "    #Contadores para avaliar o balanço de gatos e doggos no dataset\n",
    "    catcount = 0\n",
    "    dogcount = 0\n",
    "\n",
    "    #Vamos criar uma função para criar o dataset de treino\n",
    "    def make_training_data(self):\n",
    "        for label in self.LABELS:\n",
    "            print(label)\n",
    "            \n",
    "            #Iterando nas imagens dentro do diretório\n",
    "            #Obs: o tqdm() só serve pra criar uma barra de progresso bonitinha. Vc Vai ver quando rodar.\n",
    "            for f in tqdm(os.listdir(label)):\n",
    "                if \"jpg\" in f:\n",
    "                    try:\n",
    "                        #Criando o path para carregar a imagem\n",
    "                        path = os.path.join(label, f)\n",
    "                        #Carregando a imagem\n",
    "                        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "                        #Reajustando o tamanho da imagem\n",
    "                        img = cv2.resize(img, (self.IMG_SIZE, self.IMG_SIZE))\n",
    "                        \n",
    "                        #Adicionando a imagem ao dataset. Obs: Estaremos utilizando onehotvector para os labels\n",
    "                        #Para converter para onehotvector, utilizamos np.eye()\n",
    "                        #np.eye() basicamente cria uma matriz identidade do tamanho especificado\n",
    "                        self.training_data.append([np.array(img), np.eye(2)[self.LABELS[label]]])  \n",
    "\n",
    "                        if label == self.CATS:\n",
    "                            self.catcount += 1\n",
    "                        elif label == self.DOGS:\n",
    "                            self.dogcount += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "\n",
    "        #Embaralhando o dataset\n",
    "        np.random.shuffle(self.training_data)\n",
    "        #Salvando o dataset\n",
    "        np.save(\"training_data.npy\", self.training_data)\n",
    "        print('Cats:',dogsvcats.catcount)\n",
    "        print('Dogs:',dogsvcats.dogcount)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 25/12501 [00:00<00:50, 248.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PetImages/Cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12501/12501 [00:14<00:00, 884.54it/s]\n",
      "  1%|          | 78/12501 [00:00<00:16, 776.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PetImages/Dog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12501/12501 [00:14<00:00, 867.55it/s]\n",
      "/home/kodex/anaconda3/envs/pytorch/lib/python3.8/site-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cats: 12476\n",
      "Dogs: 12470\n"
     ]
    }
   ],
   "source": [
    "#O rebuild_data ficara setado para falso depois de rodarmos uma vez. \n",
    "#Assim não precisaremos carregar o dataset toda vez\n",
    "if REBUILD_DATA:\n",
    "    #Cria um objeto da classe\n",
    "    dogsvcats = DogsVSCats()\n",
    "    #Trata os dados e salva em \"traning_data.npy\"\n",
    "    dogsvcats.make_training_data()\n",
    "    #Carrega o \"training_data.npy\"\n",
    "    training_data = np.load(\"training_data.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja um como um elemento do training data set parece: Primeiro temos a matriz de pixels 50x50x1 e a seguir temos os labels 0/1 indicando de é gato ou cachorro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[197, 198, 202, ..., 232, 219, 218],\n",
      "       [200, 200, 201, ..., 228, 218, 218],\n",
      "       [202, 200, 202, ..., 229, 218, 218],\n",
      "       ...,\n",
      "       [196, 174, 166, ..., 227, 238, 228],\n",
      "       [166, 191, 221, ..., 200, 193, 147],\n",
      "       [202, 238, 229, ..., 229, 198, 137]], dtype=uint8)\n",
      " array([1., 0.])]\n"
     ]
    }
   ],
   "source": [
    "print(training_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f34f7463490>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(training_data[0][0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 6\n",
    "## A seguir, vamos separar os dados em batches e treiná-los."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Classe do modelo\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        #Chamando o init da classe pai\n",
    "        super().__init__()\n",
    "        \n",
    "        #Estruturando as camadas\n",
    "        \n",
    "        #O primeiro parâmetro é a entrada da conv layer. Em fully-connected layers, precisaríamos planarizar a \n",
    "        #imagem em um único vetor e então passar como entrada o tamanho desse vetor\n",
    "        #Aqui, a imagem é passada inteira. Desse modo, basta passar 1 como entrada, pq passaremos 1 imagem\n",
    "        \n",
    "        #O segundo parâmetro é a saída da conv layer, ou seja, é a quantidade de filtros(kernels) que serão gerados\n",
    "        #O terceiro parâmetro é o tamanho do filtro(kernel). Nesse caso, teremos 5x5\n",
    "        #Ou seja, o kernel dessa camada tem dimensões 32x5x5.\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5)\n",
    "        \n",
    "        #Vamos fazer 3 conv layers\n",
    "        #Conv2 possui um kernel 64x5x5\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5)\n",
    "        #Conv3 possui um kernel 128x5x5\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5)\n",
    "        \n",
    "        #Agora vamos usar camadas fully-connected. Para isso, temos que usar o \"flatten\"\n",
    "        #Porém, antes, devemos calcular o tamanho disso. \n",
    "        #Veja a célula abaixo para mais informações.\n",
    "        \n",
    "        self.fc1 = nn.Linear(512, 512)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, (2,2))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, (2,2))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, (2,2))\n",
    "\n",
    "        #Hora de usar o flatten\n",
    "\n",
    "        x = x.view(-1, 512)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para descobrirmos qual será o tamanho da saída da 3º conv layer para que possamos usar o flatten, devemos passar dados pela rede uma vez e ver qual é o output. Nessa primeira passagem, comentaremos nossas camadas fully-connected. Desse modo, saberemos o tamanho do output da 3º conv layer.\n",
    "\n",
    "Outra forma é calcular o tamanho da imagem até chegar na fl layer. O cálculo seria o seguinte:\n",
    "\n",
    "IMG_SIZE = 50 -> conv1(50 - 5 + 1) -> pool2x2((46-2)/2 +1) -> conv2(23 - 5 + 1) -> pool2x2((19-2)/2+1) -> conv3(9 - 5 + 1) -> pool2x2((5-2)/2+1) = 2\n",
    "\n",
    "2 * 2 * 128 = 512. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando o optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do test set:  2494\n"
     ]
    }
   ],
   "source": [
    "#Separando as features e labels do training_set\n",
    "X = torch.Tensor([i[0] for i in training_data]).view(-1, 50, 50)\n",
    "#Normalizando o valor do pixels da imagem\n",
    "X = X/255.0\n",
    "y = torch.Tensor([i[1] for i in training_data])\n",
    "\n",
    "#Separando training e test sets\n",
    "VAL_PCT = 0.1\n",
    "val_size = int(len(X)*VAL_PCT)\n",
    "print(\"Tamanho do test set: \", val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino: 22452\n",
      "Teste: 2494\n"
     ]
    }
   ],
   "source": [
    "#Pega de 0 até final-val_size\n",
    "train_X = X[ : -val_size]\n",
    "train_y = y[ : -val_size]\n",
    "\n",
    "#Pega de final-val_size até final\n",
    "test_X = X[-val_size :]\n",
    "test_y = y[-val_size :]\n",
    "\n",
    "print(\"Treino:\", len(train_X))\n",
    "print(\"Teste:\", len(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:41<00:00,  5.42it/s]\n",
      "  0%|          | 1/225 [00:00<00:43,  5.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Loss: 0.13052375614643097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:41<00:00,  5.47it/s]\n",
      "  0%|          | 1/225 [00:00<00:42,  5.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1. Loss: 0.09235759824514389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:41<00:00,  5.47it/s]\n",
      "  0%|          | 1/225 [00:00<00:43,  5.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2. Loss: 0.06425964832305908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:44<00:00,  5.11it/s]\n",
      "  0%|          | 0/225 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3. Loss: 0.033150896430015564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:43<00:00,  5.19it/s]\n",
      "  0%|          | 0/225 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4. Loss: 0.12489456683397293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:43<00:00,  5.20it/s]\n",
      "  0%|          | 1/225 [00:00<00:42,  5.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5. Loss: 0.060841210186481476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 23/225 [00:04<00:38,  5.20it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-69706b8ca1e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-89-2c3160a35278>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     return torch.max_pool2d(\n\u001b[0m\u001b[1;32m    576\u001b[0m         input, kernel_size, stride, padding, dilation, ceil_mode)\n\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 100\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    #Começa em zero e vai até o final do training set, pulando BATCH_SIZE a cada iteração\n",
    "    for i in tqdm(range(0, len(train_X), BATCH_SIZE)): \n",
    "        \n",
    "        #Separa os batches\n",
    "        batch_X = train_X[i:i+BATCH_SIZE].view(-1, 1, 50, 50)\n",
    "        batch_y = train_y[i:i+BATCH_SIZE]\n",
    "\n",
    "        #Zerando o gradiente para a loss não acumular durante o treinamento\n",
    "        net.zero_grad()\n",
    "\n",
    "        #Forward prop\n",
    "        outputs = net(batch_X)\n",
    "        #Calcula o custo\n",
    "        loss = loss_function(outputs, batch_y)\n",
    "        #Backward prop\n",
    "        loss.backward()\n",
    "        #Atualiza os pesos\n",
    "        optimizer.step()   \n",
    "        \n",
    "    print(f\"Epoch: {epoch}. Loss: {loss}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2494/2494 [00:03<00:00, 651.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Avaliando o modelo\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(test_X))):\n",
    "        real_class = torch.argmax(test_y[i])\n",
    "        net_out = net(test_X[i].view(-1, 1, 50, 50))\n",
    "        predicted_class = torch.argmax(net_out)\n",
    "        \n",
    "        if(predicted_class == real_class):\n",
    "            correct += 1\n",
    "        total += 1\n",
    "      \n",
    "    print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
